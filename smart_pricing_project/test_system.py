# MIT License
# test_system.py
"""
System validation and testing script.
"""

from __future__ import annotations
import os
import sys
import pandas as pd
import numpy as np

# Add project root to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.config import Config
from src.utils.logger import get_logger
from src.utils.seed import set_seed
from src.data.feature_store import FeatureStore

log = get_logger("test_system")
cfg = Config()

def test_data_loading():
    """Test that data can be loaded correctly"""
    log.info("[AI-Agent] Testing data loading...")
    
    # Test raw data loading
    train_path = "dataset/train.csv"
    test_path = "dataset/test.csv"
    
    if not os.path.exists(train_path):
        log.error(f"Train data not found: {train_path}")
        return False
    
    if not os.path.exists(test_path):
        log.error(f"Test data not found: {test_path}")
        return False
    
    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)
    
    log.info(f"Train data shape: {train_df.shape}")
    log.info(f"Test data shape: {test_df.shape}")
    log.info(f"Train columns: {list(train_df.columns)}")
    
    # Test feature store
    if os.path.exists("outputs/feature_store/train_features.csv"):
        fs = FeatureStore()
        train_feat = fs.load_train()
        test_feat = fs.load_test()
        log.info(f"Feature store train shape: {train_feat.shape}")
        log.info(f"Feature store test shape: {test_feat.shape}")
    
    log.info("[AI-Agent] ‚úÖ Data loading test passed")
    return True

def test_config():
    """Test configuration loading"""
    log.info("[AI-Agent] Testing configuration...")
    
    log.info(f"Config seed: {cfg.seed}")
    log.info(f"Config device: {cfg.device}")
    log.info(f"Config data_dir: {cfg.data_dir}")
    log.info(f"Config model_dir: {cfg.model_dir}")
    
    log.info("[AI-Agent] ‚úÖ Configuration test passed")
    return True

def test_imports():
    """Test that all modules can be imported"""
    log.info("[AI-Agent] Testing module imports...")
    
    try:
        from src.utils.smape_loss import smape_numpy, SMAPELoss
        from src.models.lgb_model import LGBModel
        from src.models.ensemble_stacker import EnsembleStacker
        from src.data.embeddings_text_lora import TextEmbedder
        log.info("[AI-Agent] ‚úÖ Core module imports successful")
    except Exception as e:
        log.error(f"Import failed: {e}")
        return False
    
    return True

def test_smape_calculation():
    """Test SMAPE calculation"""
    log.info("[AI-Agent] Testing SMAPE calculation...")
    
    from src.utils.smape_loss import smape_numpy
    
    # Test cases
    y_true = np.array([10.0, 20.0, 30.0])
    y_pred = np.array([11.0, 18.0, 32.0])
    
    smape_score = smape_numpy(y_true, y_pred)
    log.info(f"SMAPE test score: {smape_score:.4f}%")
    
    # Should be reasonable (< 20% for this example)
    if smape_score < 20.0:
        log.info("[AI-Agent] ‚úÖ SMAPE calculation test passed")
        return True
    else:
        log.error("SMAPE calculation may be incorrect")
        return False

def test_text_embedding_fallback():
    """Test text embedding with fallback"""
    log.info("[AI-Agent] Testing text embedding fallback...")
    
    try:
        from src.data.embeddings_text_lora import TextEmbedder
        
        # Test with small sample
        texts = ["sample product description", "another product text"]
        
        te = TextEmbedder(use_lora=False)  # Use fast mode
        
        # This should work if sentence-transformers is available
        try:
            embeddings = te.embed(texts, mode="fast")
            log.info(f"Text embeddings shape: {embeddings.shape}")
            log.info("[AI-Agent] ‚úÖ Text embedding test passed")
            return True
        except Exception as e:
            log.warning(f"Text embedding failed (expected if dependencies missing): {e}")
            return True  # This is okay, we have fallbacks
            
    except Exception as e:
        log.error(f"Text embedding module failed: {e}")
        return False

def validate_submission_format():
    """Validate submission format"""
    log.info("[AI-Agent] Testing submission format validation...")
    
    # Create a dummy submission
    sample_ids = [1, 2, 3, 4, 5]
    prices = [10.5, 20.0, 15.7, 8.3, 25.1]
    
    submission_df = pd.DataFrame({
        'sample_id': sample_ids,
        'price': prices
    })
    
    # Validation checks
    assert 'sample_id' in submission_df.columns, "Missing sample_id column"
    assert 'price' in submission_df.columns, "Missing price column"
    assert submission_df['price'].isnull().sum() == 0, "NaN values in price"
    assert (submission_df['price'] > 0).all(), "Non-positive prices"
    
    log.info("[AI-Agent] ‚úÖ Submission format validation passed")
    return True

def main():
    """Run all tests"""
    set_seed(cfg.seed)
    log.info("[AI-Agent] üß™ Starting system validation tests")
    
    tests = [
        ("Configuration", test_config),
        ("Module Imports", test_imports),
        ("Data Loading", test_data_loading),
        ("SMAPE Calculation", test_smape_calculation),
        ("Text Embedding Fallback", test_text_embedding_fallback),
        ("Submission Format", validate_submission_format)
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            if result:
                passed += 1
            else:
                failed += 1
                log.error(f"‚ùå {test_name} test failed")
        except Exception as e:
            failed += 1
            log.error(f"‚ùå {test_name} test failed with exception: {e}")
    
    log.info(f"[AI-Agent] üìä Test Results: {passed} passed, {failed} failed")
    
    if failed == 0:
        log.info("[AI-Agent] üéâ All tests passed! System is ready for training.")
    else:
        log.warning(f"[AI-Agent] ‚ö†Ô∏è Some tests failed. Review before full training.")
    
    return failed == 0

if __name__ == "__main__":
    main()